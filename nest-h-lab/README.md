## การจัดการ Feature ใหม่ให้เป็น Near-Realtime

การจัดการ Feature ใหม่ให้เป็น near-realtime โดย stack ปัจจุบันเป็น RESTAPI และ RDBMS database ที่เรียกใช้งาน microservice api ของ Transaction data api, Customer api และ Master data api โดยไม่เปลี่ยนแปลง api ทั้งสามตัว เราสามารถนำเอา message brocker โดย Nestjs เองก็มีตัวของ RabbitMQ หรือ Kafka และ Redis เพื่อทำ caching

### Message Broker (RabbitMQ / Kafka)

การใช้ message broker อย่าง RabbitMQ หรือ Kafka ช่วยให้สามารถส่งข้อมูลระหว่าง service ต่างๆ ได้แบบ asynchronous โดยไม่รบกวนการทำงานของ service หลัก ซึ่งเหมาะสมสำหรับการจัดการข้อมูลแบบ near-realtime ถูกใช้เพื่อส่งข้อความหรืออัปเดตข้อมูลระหว่าง microservices อื่นๆ

### Redis Caching

Redis เพื่อทำ caching ได้โดย  
สามารถใช้ Redis เพื่อเก็บข้อมูลที่ถูกเรียกใช้บ่อยๆ หรือเก็บผลลัพธ์จากการประมวลผล เพื่อไม่ต้องไปดึงข้อมูลจากฐานข้อมูลหรือ API ทุกครั้ง

---

## การวางแผนการทดสอบประสิทธิภาพ (Performance Test) สำหรับโปรเจคใหม่

### 1. Objective
1.1 **จำนวนผู้ใช้งานจำนวนมากในเวลาเดียวกัน**: ทดสอบระบบที่รองรับผู้ใช้จำนวนมากพร้อมกัน (เช่น 5,000 - 50,000 คน) ในช่วงเวลาเดียวกัน  
1.2 **Response time ในช่วงที่มีการทำงานที่หนัก**: วัดเวลาในการตอบสนองของระบบเมื่อมีการโหลดสูง  
1.3 **Stress Testing**: ทดสอบความสามารถในการรับมือกับโหลดที่สูงเกินไป เพื่อดูว่าระบบสามารถรับภาระได้มากน้อยเพียงใด  

### 2. ขอบเขตการทดสอบ (Scope)
2.1 **ทดสอบ End point ของทั้งระบบ** หรือ **ทดสอบส่วนไหนเป็นพิเศษ**: เช่น ระบบการสั่งซื้อสินค้า หรือฟังก์ชันที่มีความสำคัญสูง  
2.2 ระบุจุดที่ต้องการทดสอบเพิ่มเติม เช่น การเชื่อมต่อฐานข้อมูล, การทำงานของ API, หรือกระบวนการที่เกี่ยวข้องกับการประมวลผลข้อมูลหนักๆ  

### 3. ประเภทของการทดสอบที่ต้องการ
3.1 **ทดสอบการเพิ่มขึ้นของผู้ใช้จาก 5,000 คนเป็น 50,000 คนในเวลา 5 นาที**: เพื่อวัดความสามารถในการขยายตัวของระบบ  
3.2 **จำลองการใช้งาน 200,000 ผู้ใช้ในเวลาเดียวกัน**: เพื่อทดสอบขีดจำกัดของระบบในสภาวะที่มีผู้ใช้จำนวนมาก  
3.3 **Stress Testing**: การทดสอบระบบเมื่อมีการใช้งานเกินขีดจำกัด เช่น การจำลองผู้ใช้จำนวนมากเกินความสามารถ  

### 4. การจำลองสภาวะการทดสอบ
4.1 **การทำ Server จำลอง**: ใช้บริการเช่น **AWS EC2** สำหรับการจำลองการทำงานของเซิร์ฟเวอร์ โดยสามารถปรับเพิ่มหรือลดขนาดเครื่องได้ตามต้องการ  
4.2 การจำลองปริมาณโหลดที่มีความหลากหลาย เช่น การจำลองการเข้าใช้งานหลายๆ โซนหรือแหล่งที่มาของการเข้าใช้งาน (ต่างประเทศ/ต่างภูมิภาค)  

### 5. เครื่องมือที่ใช้ในการทดสอบ
5.1 **Apache JMeter**: สำหรับการทดสอบการโหลด เช่น การจำลองการใช้งานที่มีการขอข้อมูลจาก API หรือการทดสอบปริมาณผู้ใช้ที่เข้าใช้งานพร้อมกัน  
5.2 **Gatling**: เครื่องมือสำหรับการทดสอบ performance ที่สามารถใช้ในการทดสอบ API ได้อย่างมีประสิทธิภาพ  

### 6. ข้อมูลที่ต้องการเก็บระหว่างการทดสอบ
6.1 **การใช้ CPU และหน่วยความจำของระบบ**: ตรวจสอบการใช้งานของทรัพยากร เช่น CPU, RAM, Disk I/O ในระหว่างการทดสอบเพื่อประเมินประสิทธิภาพของเซิร์ฟเวอร์  
6.2 **เวลาในการตอบสนองของระบบ**: ตรวจสอบว่าเวลาตอบสนองของ API หรือการดำเนินการต่างๆ อยู่ในขอบเขตที่กำหนดไว้หรือไม่  
6.3 **Error Rate**: อัตราการเกิดข้อผิดพลาดที่เกิดขึ้นระหว่างการทดสอบ  
6.4 **Throughput**: จำนวนการร้องขอที่ระบบสามารถจัดการได้ในช่วงเวลาที่กำหนด  

### 7. เกณฑ์ที่ระบบต้องสามารถทำได้เพื่อให้การทดสอบผ่าน
7.1 **เวลาตอบสนองไม่เกิน X วินาที**: ระบุเวลาในการตอบสนองที่เหมาะสมเพื่อให้การทดสอบผ่าน เช่น ระบบต้องตอบสนองไม่เกิน 2 วินาทีต่อการร้องขอ  
7.2 **รองรับผู้ใช้จำนวน Y คนโดยที่เซิร์ฟเวอร์ไม่ล้ม**: ระบบต้องสามารถรองรับผู้ใช้พร้อมกัน 50,000 คนได้โดยไม่ล้ม  
7.3 **Error rate ไม่เกิน Z%**: เกณฑ์ในการทดสอบอัตราผิดพลาด เช่น ระบบสามารถรองรับได้ไม่เกิน 1% ของผู้ใช้ที่มีปัญหาหรือเกิดข้อผิดพลาด  

### 8. ความเสี่ยงที่อาจเกิดขึ้นระหว่างการทดสอบ
8.1 **Server Overload**: หากระบบไม่สามารถรองรับจำนวนผู้ใช้ที่ทดสอบได้ จะทำให้ระบบล้มเหลวหรือเกิดอาการช้าลง  
8.2 **ข้อผิดพลาดใน API**: หาก API มีการจัดการข้อผิดพลาดไม่ดีอาจทำให้ระบบหยุดทำงานหรือเกิดผลกระทบต่อบริการ  
8.3 **Resource Bottleneck**: ความไม่สมดุลของทรัพยากร เช่น CPU, RAM หรือ Disk I/O อาจทำให้ระบบทำงานช้าหรือไม่สามารถรองรับการใช้งานได้  

### 9. การรายงานผลลัพธ์
9.1 **รูปแบบการรายงาน**: รายงานผลลัพธ์ต้องรวมถึงข้อมูลที่สำคัญ เช่น เวลาในการตอบสนอง, อัตราความผิดพลาด, จำนวนผู้ใช้ที่รองรับได้, และการใช้ทรัพยากรของระบบ  
9.2 **รายงานที่ไหน**: การรายงานผลลัพธ์สามารถทำได้ในรูปแบบ **PDF** หรือ **HTML** และสามารถนำเสนอใน **Dashboard** เช่น **Grafana** เพื่อแสดงผลแบบเรียลไทม์  
9.3 รายงานจะต้องมีสรุปการทดสอบ พร้อมข้อเสนอแนะหรือการปรับปรุงที่จำเป็น

---

## Multilingual Product API

ระบบ **Multilingual Product API** ผมออกแบบเพื่อรองรับการแปลภาษาในข้อมูลสินค้าโดยใช้ Gemini 1.5 ซึ่งเป็นเครื่องมือที่ไม่เสียค่าใช้จ่ายและไม่มีข้อจำกัดเรื่องการใช้ token

ซึ่งจะต้องใช้งาน ENV ดังนี้

```
PORT=
DATABASE_URL=
GEMINI_API_KEY=
```

Gemini API key ได้มาจาก https://aistudio.google.com/app/apikey

เนื่องจาก database บางส่วนอาจจะต้องกรอกเอง หากสามารถลดความยุ่งยากสามารถใช้ database นี้ได้เลยครับ

postgres://neondb_owner:npg_8mcTLUQsMdh5@ep-polished-dust-a1dv7jhm-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require

**โดยภาษาที่รองรับเป็นจีนและอังกฤษ**

```
{
    "name": "หูฟังไร้สาย",
    "language": ["ch", "en"],
    "description": "หูฟังไร้สายคุณภาพสูงพร้อมระบบตัดเสียงรบกวน"
}
```

### การออกแบบฐานข้อมูล:
- **Product Table**: ตารางหลักที่ใช้เก็บข้อมูลสินค้า
- **Language Table**: ตารางที่เก็บภาษาที่ต้องการให้มีในระบบ
- **Product Dictionary Table**: เก็บข้อมูลการแปลของแต่ละภาษา

การออกแบบนี้รองรับการเปลี่ยนแปลงในอนาคต โดยสามารถลบภาษาที่ไม่ต้องการใช้งานออกได้โดยไม่เกิดปัญหา

![ER Diagram](./assets/h-lab-diagram.png)

---

### การตรวจสอบข้อมูล (Input Validation):
การตรวจสอบข้อมูลขาเข้าจะใช้ `class-validator` ของ NestJS ที่มีฟังก์ชันการตรวจสอบข้อมูลอยู่แล้ว ซึ่งจะช่วยให้ข้อมูลถูกต้องก่อนที่จะถูกส่งไปยัง Service หรือที่เรียกว่า DTO

---

### กลยุทธ์การทดสอบ

#### Unit Testing:
การทดสอบ Unit เน้นการทดสอบส่วนที่เป็นการตอบกลับ เช่น การทดสอบกรณีที่ไม่มีข้อผิดพลาดและกรณีที่เกิดข้อผิดพลาด เช่น การตอบกลับสถานะ 400 เมื่อข้อมูลผิด หรือ 500 เมื่อเกิดข้อผิดพลาดจากเซิร์ฟเวอร์

**คำสั่ง**: `npm run test`  
**ไฟล์ที่ใช้**: `product.controller.spec.ts`

#### Integration Testing:
การทดสอบ Integration เน้นการทดสอบการทำงานร่วมกันของ repositories และ services ต่าง ๆ เพื่อให้แน่ใจว่าไม่มีข้อผิดพลาดในการทำงานร่วมกัน

**คำสั่ง**: `npm run test`  
**ไฟล์ที่ใช้**: `product.service.spec.ts`

#### End-to-End (E2E) Testing:
การทดสอบ End-to-End (E2E) ผมทำ test case senario ตั้งแต่การสร้างข้อมูล (create) จนถึงการดึงข้อมูล (get) ผ่าน API โดยจะทดสอบการ reponse ของ API ทั้งหมด รวมถึง status code รูปแบบของ response และข้อมูลที่ส่งกลับมา

**คำสั่ง**: `npm run test:e2e`  
**ไฟล์ที่ใช้**: `product.e2e-spec.ts`

---
### API DOSC
```
http://localhost:3000/api-docs
```
